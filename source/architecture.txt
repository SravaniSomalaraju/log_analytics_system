Phase 6 - Hadoop Architecture Evolution Study

Task 7 - Architecture Comparison

Hadoop 1.x Architecture

Components:
NameNode -> Stores metadata
DataNode -> Stores actual data blocks
JobTracker -> Handles job scheduling + resource management
TaskTracker -> Executes map and reduce tasks

JobTracker & TaskTracker Limitations:
Single Point of Failure:-
-If JobTracker fails -> entire cluster stops.
Scalability Issues:-
-JobTracker manages both resource allocation and job scheduling → becomes bottleneck in large clusters.
Poor Resource Utilization:-
-Only MapReduce supported -> cannot run other processing frameworks.
Limited Cluster Size:-
#-Not suitable for very large distributed systems.


Hadoop 2.x Architecture (Introduction of YARN)
Hadoop 2.x introduced YARN (Yet Another Resource Negotiator) to overcome Hadoop 1.x limitations.

YARN separates:
Resource management
Job execution

ResourceManager:
-Global cluster resource manager
-Allocates resources to applications
-Performs scheduling
NodeManager:
-Runs on each node
-Manages containers and resources
-Executes tasks
-Reports node status

ApplicationMaster:
-Created per application/job
-Negotiates resources from ResourceManager
-Manages execution of that specific job

Improvements over Hadoop 1.x:
-Removed JobTracker bottleneck
-Better scalability
-Multiple processing engines supported (MapReduce, Spark, Tez, etc.)
-Improved resource utilization
-Better fault tolerance

Hadoop 3.x Architecture (Modern Enhancements)
Hadoop 3.x builds on YARN with major improvements.

Key Enhancements:
High Availability (HA):
-Multiple NameNodes -> Active + Standby
-Automatic failover -> no single point of failure
Improved Scalability:
-Supports very large clusters
-Better resource scheduling and container management
Erasure Coding:
-Reduces storage overhead compared to replication
-More efficient storage utilization
Multiple Resource Types:
-Supports CPU, memory, GPU scheduling




Phase 7 - Hadoop Configuration and Performance Tuning

Task 8 - Configuration Analysis

When log volume increases, Hadoop performance may degrade due to improper configuration of HDFS, MapReduce, and YARN. Performance tuning focuses on optimizing resource usage, block handling, and job execution.

1. HDFS Block Management

Configuration Files:
- core-site.xml → General Hadoop settings (filesystem, I/O)
- hdfs-site.xml → HDFS storage and block management
- mapred-site.xml → MapReduce execution and memory
- yarn-site.xml → Resource allocation and scheduling

Critical Properties:

File: hdfs-site.xml:-
dfs.blocksize:-
Defines size of each HDFS block (default = 128 MB)
Recommendation:
Keep block size 128 MB or increase to 256 MB for very large files
Impact:
Larger block -> fewer blocks -> less metadata load on NameNode
Improves sequential read performance
Too large -> reduces parallel processing

dfs.replication:-
Number of copies of each block (default = 3)
Recommendation (single node): Set to 1
Impact:
Lower replication -> less storage usage , faster write
Higher replication -> better fault tolerance but more storage

dfs.namenode.handler.count
Number of threads handling HDFS requests
Recommendation: Increase if many concurrent operations
Impact:
Handles more concurrent requests
Reduces NameNode bottleneck

2. MapReduce Execution

File: mapred-site.xml

mapreduce.map.memory.mb
Memory for each mapper
Recommendation: Increase based on system RAM (e.g., 1024 -> 2048)
Impact:
Prevents mapper failures
Improves processing of large records

mapreduce.reduce.memory.mb
Memory per each reducer
Recommendation: Increase for heavy aggregation jobs
Impact:
Faster reduce phase
Prevents out-of-memory errors

mapreduce.task.io.sort.mb
Buffer size for map output before disk write
Recommendation: Increase (e.g., 100 -> 256)
Impact:
Reduces disk I/O
Improves shuffle performance

3. YARN Resource Allocation
File: yarn-site.xml

yarn.nodemanager.resource.memory-mb
Total memory available for containers on each node
Recommendation: Set close to system RAM (leave some for OS)
Impact:
More containers can run
Better resource utilization

yarn.scheduler.maximum-allocation-mb
Max memory for a single container
Recommendation: Increase for large jobs
Impact:
Allows memory-intensive tasks
Prevents container rejection

yarn.nodemanager.resource.cpu-vcores
CPU cores available to YARN
Recommendation: Match system CPU cores
Impact:
Enables more parallel tasks
Improves throughput


Impact on Job Execution and Resource Usage
- Faster job execution due to better memory and reduced disk I/O
- Improved parallelism from optimized block size and CPU allocation
- Better resource utilization (CPU, memory, disk)
- Reduced bottlenecks at NameNode and during shuffle phase